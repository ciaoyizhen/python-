# 第二十一章 大规模数据处理

人类是数据的创造者和使用者，自结绳记事起，数据就慢慢产生。随着计算机和互联网的广泛应用，人类产生、创造的数据量呈爆炸式增长。中国已成为全球数据总量最大，数据类型最丰富的国家之一。仿佛一夜之间，所有人都开始讨论大数据了，企业都都在讨论hadoop、Hive、MapReduce等。那么面对大规模数据时，我们使用pandas能处理吗？

## 不同规模数据处理工具的选择

不知道为什么，个人感觉我们做事总是一窝蜂而上。突然之间，所有人都在讨论大数据；之后大家又言必称机器学习、人工智能、深度学习；再后来，各个企业都说自己在作区块链。对于pandas也一样，许多人还没有将其研究透，就觉得pandas无法处理大规模数据，应该学习一个新的工具了。很多企业在没有相应技术储备，同时自己数据量并不大时，盲目追求大，反而是一种灾难。

+ 如果数据只有几十兆字节(MB)，也许使用Excel、pandas就是不错的选择。
+ 如果数据达到几十兆字节(MB)到几吉字节(GB)，此时pandas或数据库都是不错的选择，pandas相比数据库有时还具灵活性与高效性。
+ 如果数据规模从几吉字节(GB)到几十吉字节(GB)，此时pandas和数据库都能胜任。
+ 如果数据已经超过几百吉字节(GB)，进入TB级，此时开始考虑采用大数据系统无疑是不错的选择。

假设现在有一个数据集，一共20列，每列需要40B来存储，那么存储一行需要800B，存储一百万行数据需要760M左右，存储一千万行不到8GB。

## 利用pandas处理大规模数据

### 文件分块读入

由于pandas是将数据文件读入到内存中再进行分析，于是很多人机会认为内存大小决定了能处理的数据大小。其实我们可以进行变通，如将文件分块读入处理

```python
import pandas as pd
csv_file = r'../data/gapminder.csv'
chunksize=500
for gm_chunk in pd.read_csv(csv_file, chunksize=chunksize):
    print(gm_chunk.shape)
    
    
dataiter = pd.read_csv(csv_file, chunksize=chunksize)
a = next(dataiter)
a
```

我们可以把文件数据分批读入，然后单独处理，最后再合并进行处理。

### 使用数据库

对于数据文件超过内存，除了使用pandas分块读入分析的方法，还可以考虑与数据库结合来应对文件大小超过系统内存的情况。

这里略（暂时不会）

```

```

### 使用DASK

DASK是一款用于分析计算的灵活并行计算库，在进行大规模的数据分析时，本机内存往往不够，同时又不想使用Spark等大数据工具的话，DASK是一个不错的替代选择。通常DASK有两种应用场景，一种是利用dask.array，dask.Dataframe来分析大型数据集，这与数据库、Spark等类似；另一种应用场景是自定义任务计划。

这里介绍第一种

```python
import dask.dataframe as dd
df = dd.read_csv('../data/gapminder.csv')

df  # 单独这样，只能看到列表和一行结构说明
df.head()  # 和DataFrame一致

radiator_df = df[df['time']==1700]  # 这里的操作，只会返回DASK对象
radiator_df.compute()  # 只有经过了compute()， 才会返回pandas结果
```

